### Study Notes: Language Modeling with Decoders and GPT-like Models

#### Overview of Decoders in Transformers

- **Transformers**:
  - Initially designed for language translation tasks.
  - Composed of two key components:
    1. **Encoder**: Processes input text.
    2. **Decoder**: Generates output text.
- **Evolution of Decoders**:
  - Decoders are central to text generation.
  - They form the foundation of sophisticated models like GPT, LLaMA, and Granite.

#### Decoder Models for Text Generation

- **Generative Pre-training (GPT)**:
  - A self-supervised approach.
  - Trains a decoder to predict the next token or word in a sequence.
- **Autoregressive Models**:
  - Predict new tokens by examining preceding tokens.
  - Sequential generation ensures that the model adheres to context from prior inputs.

#### Training Methods for Decoders

- **Fine-Tuning**:
  - A supervised process that tailors pre-trained models to specific tasks, e.g., question-answering (QA) or classification.
- **Reinforcement Learning from Human Feedback (RLHF)**:
  - Enhances model performance on specific tasks, such as chatbots.
  - A fine-tuning technique that incorporates human feedback.

#### Key Characteristics of Decoders

- Operate **without an encoder's input** during text generation.
- Rely on **masked self-attention** to focus only on preceding tokens when making predictions.

#### Process of Text Generation

1. **Input Prompt**:
   - For example, a user prompt like "How are you?".
   - Models tokenize the input and add a special "beginning of sentence" (BOS) token if required.
2. **Tokenization and Embedding**:
   - Converts input text into tokens.
   - Tokens are converted into **word embedding vectors**.
3. **Sequence Prediction**:
   - Autoregressive process predicts the next token based on the input sequence.
   - The predicted token is appended to the sequence for further predictions.
   - Continues until a stopping condition, e.g., reaching an end-of-sequence (EOS) token.
4. **Contextual Embeddings**:
   - Generated by processing word embeddings in the context of the sequence.
   - Reflect the relationship between words in the sequence.
   - Unlike static embeddings, contextual embeddings change depending on the surrounding words.

#### Example: Generating a Response

- Prompt: "How are you?"
  - BOS token is added.
  - Input is tokenized into ["how", "are", "you"].
  - Word embeddings are computed and transformed.
- Prediction Process:
  - The decoder predicts "good" as the next token.
  - The updated input becomes "How are you? Good."
  - The model then predicts "thanks" as the next token.
  - This process repeats until an EOS token or another stopping condition is met.

#### Attention Mechanisms in Decoders

- **Masked Self-Attention**:
  - Ensures the model only attends to previous tokens, preserving the autoregressive nature.
- **Matrix Multiplication**:
  - Core operation of the attention mechanism.
- **Masking in Training and Inference**:
  - Used during both training and inference to maintain the autoregressive property.

#### Decoder Components

- **Word Embeddings**:
  - Represent words as vectors.
- **Positional Encodings**:
  - Add information about the position of words in a sequence.
- **Contextual Embeddings**:
  - Result from transforming input embeddings based on context.
  - Passed through a linear layer to generate logits.
- **Logits**:
  - Outputs of the model, used to predict the next token using an argmax operation.

#### Differences Between Encoders and Decoders

- **Encoders**:
  - Focus on understanding input sequences.
  - Use full self-attention mechanisms.
- **Decoders**:
  - Focus on generating output sequences.
  - Use masked self-attention to ensure sequential generation.

#### Summary of Key Concepts

- Decoders are fundamental in text generation tasks.
- GPT models are based on autoregressive decoders.
- Fine-tuning and RLHF improve the model's ability to perform specialized tasks.
- The decoder's ability to use masking and attention mechanisms facilitates accurate sequence generation.
