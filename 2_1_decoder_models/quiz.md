1.
Question 1
In transformer architectures, excluding the translation task, what is the principal distinction between encoders and decoders?


Decoders include a multi-head attention mechanism, but encoders do not.



Decoders leverage an attention mechanism involving matrix multiplication.



Encoders generate text by predicting the previous tokens in a sequence.



Encoders apply a SoftMax function before generating the output.


1 point
2.
Question 2
Which of the following statements is true for the characteristics of a decoder model implemented using PyTorch with a causal language model (LM) architecture?


The decoder models in PyTorch using causal LM initially focus on understanding the context from bidirectional dependencies within a sequence. 



The decoder models in PyTorch use causal LM relays on input embeddings without considering the order of tokens in the sequence. 



The decoder models in PyTorch using causal LM help to generate sequence-to-sequence tokens while attending to the previous tokens and ensuring coherence in sequence generation.



The decoder models in PyTorch using causal LM restrict attending sequential data due to its limitations in capturing temporary tokens.


1 point
3.
Question 3
Which of the following best describes the purpose of using decoder model neural network architecture?


Preprocess the input data before feeding it to the neural network.



Initialize the weights of the neural network parameters.



Reduce the dimensionality of the data for faster computation.



Generate outputs from the learned representations of the input data.


1 point
4.
Question 4
Which of the following is one of the advantages of implementing causal attention masking in natural language processing (NLP)?


Language translation



Image classification 



Sentiment analysis



Speech recognition


1 point
